{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taylor_swift_cleaned.pkl\n",
      "the_killers_cleaned.pkl\n",
      "the_notorious_b.i.g._cleaned.pkl\n",
      "u2_cleaned.pkl\n",
      "warren_zevon_cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get list of lyrics dataframes\n",
    "train_dir = r'genres/rap/training'\n",
    "dataframes = []\n",
    "for file in os.listdir(train_dir):\n",
    "    print(file)\n",
    "    path = train_dir + file\n",
    "    lyrics_df = pd.read_pickle(path)\n",
    "    dataframes.append(lyrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all lyrics into one string\n",
    "def generate_corpus(dataframe):\n",
    "    print('Compiling corpus from lyrics dataframe...')\n",
    "    text = ''\n",
    "    for i, lyrics in dataframe['lyrics'].iteritems():\n",
    "        text += ' ' + lyrics\n",
    "    text = text.lower()\n",
    "    print('Corpus length:', len(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_text_for_training(text, maxlen, step):\n",
    "    print('Prepping corpus for training...')\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    print(chars)\n",
    "    print('Unique characters:', len(chars))\n",
    "    char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "        \n",
    "    return chars, char_indices, sentences, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(chars, char_indices, sentences, next_chars, maxlen=60):\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    print('Done.')\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "def get_model(maxlen=60):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.LSTM(128, input_shape=(maxlen, len(chars)), dropout=0.1, recurrent_dropout=0.1))\n",
    "    # model.add(layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))\n",
    "    # model.add(layers.LSTM(64, dropout=0.1, recurrent_dropout=0.1))\n",
    "    model.add(layers.Dense(len(chars), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train_model(model, x, y, path):\n",
    "    print('Starting model training...')\n",
    "    history = model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=60,\n",
    "              callbacks=[\n",
    "                  EarlyStopping(monitor='loss', patience=5, min_delta=0.001),\n",
    "                  ModelCheckpoint(path, monitor='loss', save_best_only=True)\n",
    "              ], verbose = 0)\n",
    "    print('Done.')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling corpus from lyrics dataframe...\n",
      "Corpus length: 729632\n",
      "Prepping corpus for training...\n",
      "Number of sequences: 243191\n",
      "[' ', '!', '\"', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Unique characters: 43\n",
      "Vectorization...\n",
      "Done.\n",
      "Starting model training...\n",
      "WARNING:tensorflow:From C:\\Users\\ender\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Done.\n",
      "taylor_swift training loss: [2.35656764559734, 2.0055621348081827, 1.864114541123853, 1.7768158455210425, 1.709933022859906, 1.6625394784148, 1.6207811586971281, 1.585275305469367, 1.5530419549956291, 1.5263954220004543, 1.5054550756854368, 1.4852054339312324, 1.4679825600056373, 1.4498678819554915, 1.4378184092339383, 1.4234697580760067, 1.4136766711739623, 1.4012933684213216, 1.3890852486389564, 1.3782687690588402, 1.3696436609298095, 1.3605673164074723, 1.3549524885801147, 1.345952996481536, 1.3397582426821193, 1.3358429288948055, 1.328073930673887, 1.3226592288794181, 1.3168110178347032, 1.3120830432290398, 1.3057186507454606, 1.298562375986763, 1.296264052387621, 1.287898182609648, 1.2873125056157027, 1.285297665820279, 1.2795294501185581, 1.2758722492422188, 1.2732888249290264, 1.268641463616852, 1.2654941899893977, 1.2627766947458812, 1.2594608146558024, 1.2539500502750915, 1.2560223209171726, 1.2505950817216607, 1.2464104081601544, 1.2451346756073343, 1.2443236277484016, 1.2384690606364825, 1.2347138120736216, 1.2344313618225462, 1.2324245567141703, 1.2302307445591172, 1.231091459505747, 1.2231669032147383, 1.223438986095549, 1.2212795594453927, 1.2207988289039793, 1.2184698331582073]\n",
      "Compiling corpus from lyrics dataframe...\n",
      "Corpus length: 166767\n",
      "Prepping corpus for training...\n",
      "Number of sequences: 55569\n",
      "[' ', '!', '\"', \"'\", ',', '.', '0', '1', '2', '4', '6', '8', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Unique characters: 39\n",
      "Vectorization...\n",
      "Done.\n",
      "Starting model training...\n",
      "Done.\n",
      "the_killers training loss: [2.7228703053701357, 2.3274812984240016, 2.2091533678385313, 2.130476828521949, 2.067239828295005, 2.0168152276323514, 1.9700326131992196, 1.9329177388708296, 1.8996177308149975, 1.8745949119632468, 1.8436269010882498, 1.8197219977613708, 1.7959154843297243, 1.7728654724488977, 1.7533837561408367, 1.7353737141105265, 1.7136075279081073, 1.6964562859765446, 1.6827783332678246, 1.6673419816349526, 1.651076010543559, 1.633344350201982, 1.6247762546194522, 1.608047857429822, 1.599115289335157, 1.5841834351376693, 1.5726856601491606, 1.5610091303987028, 1.5508670133920448, 1.5403713542124617, 1.5317023990800802, 1.5237249500234191, 1.5145323555423476, 1.5024503861790128, 1.4947795500052785, 1.4856042452904028, 1.47947384233397, 1.4695586611357943, 1.4663844671001227, 1.4619559633864634, 1.4554197844808716, 1.4392585215143525, 1.436030325664079, 1.4276081881237985, 1.4237834659380701, 1.4158373076755835, 1.4136165227650432, 1.4115897155155146, 1.3988093883536579, 1.3943506927126743, 1.3876708974725673, 1.3828128709777474, 1.3750885591436766, 1.374271639950552, 1.366137661802867, 1.3613004428520148, 1.3582207685381957, 1.3497338941732295, 1.3564046488059822, 1.350107840789431]\n",
      "Compiling corpus from lyrics dataframe...\n",
      "Corpus length: 436146\n",
      "Prepping corpus for training...\n",
      "Number of sequences: 145362\n",
      "[' ', '!', '\"', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Unique characters: 43\n",
      "Vectorization...\n",
      "Done.\n",
      "Starting model training...\n",
      "Done.\n",
      "the_notorious_b.i.g. training loss: [2.597184703916287, 2.2926534620214887, 2.165646468124603, 2.0718807661727388, 1.9920037290465105, 1.9231947715047468, 1.8695053670394566, 1.8314081315344362, 1.7931881664664673, 1.766617551703263, 1.7399486006934113, 1.7217909922858086, 1.7019609217074205, 1.6809931276992072, 1.6641590431556266, 1.6508447214178075, 1.6356663277047476, 1.6272925351285736, 1.6159691530384646, 1.6049555520287972, 1.5964869961715165, 1.5886688952785106, 1.5784232660610775, 1.5739877428022215, 1.5620682990222468, 1.5560561526046552, 1.550137779493583, 1.5453628566212916, 1.5379564012557199, 1.5328576725230374, 1.527556048282164, 1.5203476164337397, 1.5144881629876272, 1.5113939638781158, 1.5074487403657404, 1.5050768772816794, 1.497247721071598, 1.49435990600144, 1.496539233829157, 1.4892988665138913, 1.4851192260565627, 1.4821358584122288, 1.4776481463429858, 1.4779266278542758, 1.4689283953002863, 1.4704218184445583, 1.4654157694013836, 1.4624331139440268, 1.4590673537898002, 1.459426979721679, 1.4534620434663725, 1.4535822757323973, 1.4522862023179863, 1.4467747409687401, 1.4470522223954267, 1.442996325233515, 1.4421648905510518, 1.4409875982758777, 1.438660957248355, 1.4349784112603496]\n",
      "Compiling corpus from lyrics dataframe...\n",
      "Corpus length: 391132\n",
      "Prepping corpus for training...\n",
      "Number of sequences: 130358\n",
      "[' ', '!', '\"', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Unique characters: 43\n",
      "Vectorization...\n",
      "Done.\n",
      "Starting model training...\n",
      "Done.\n",
      "u2 training loss: [2.516509946655793, 2.181438709242148, 2.059041692149846, 1.9733660135980409, 1.9066370793266374, 1.8491785141207993, 1.7973542357085863, 1.7590077342986914, 1.720291326624268, 1.6941729679432342, 1.6640142765365453, 1.640097188198617, 1.619861389044508, 1.5993190045282055, 1.5805528094318273, 1.5656432102782167, 1.550017182046655, 1.532237671417977, 1.5228939497192306, 1.5086343764845447, 1.4984610007323134, 1.4889549609258754, 1.4791438929401344, 1.4724543394656364, 1.4611746219506017, 1.4516273471847083, 1.4443678821438533, 1.4402003800979273, 1.431839765542034, 1.4232760967547915, 1.419106425143325, 1.4115255718779594, 1.4077809779268236, 1.398054614552701, 1.3958260910755986, 1.3902737879921743, 1.3841956598453373, 1.3844302154135661, 1.3763318343176363, 1.371620316145787, 1.3704597255107798, 1.362568466064452, 1.358987722438697, 1.3561659474421075, 1.3516501388566777, 1.3522393230603336, 1.349011498588419, 1.342335255574843, 1.335716787158083, 1.3395899732063248, 1.3327499105553602, 1.3295385220594413, 1.3290566062286746, 1.322833334878925, 1.3178560363331708, 1.3197746325926263, 1.3165050740566022, 1.3145758102117893, 1.3134159347813088, 1.3100126367755904]\n",
      "Compiling corpus from lyrics dataframe...\n",
      "Corpus length: 193189\n",
      "Prepping corpus for training...\n",
      "Number of sequences: 64377\n",
      "[' ', '!', '\"', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Unique characters: 43\n",
      "Vectorization...\n",
      "Done.\n",
      "Starting model training...\n",
      "Done.\n",
      "warren_zevon training loss: [2.738665621787713, 2.4039766898438826, 2.29775121297667, 2.223927446233631, 2.1666144005010923, 2.1161721432618434, 2.075285505633327, 2.035489433455438, 2.0002439708058795, 1.964622318951128, 1.9343465707441232, 1.9075917114958094, 1.880761396791035, 1.8589051818815066, 1.8339137053955212, 1.8160589397502622, 1.798550414158006, 1.7759800977712241, 1.7578976220415896, 1.7434052874609878, 1.7309214892383613, 1.7144551354970645, 1.7045980497255169, 1.6900583588866083, 1.6781699644607646, 1.6643048261483977, 1.6522694030897533, 1.6425462366733687, 1.6273019711676524, 1.6249794541770077, 1.6132890197654741, 1.6060585980498525, 1.5952376506830663, 1.596701056590564, 1.5790675463069008, 1.5690606532874667, 1.5673683448527194, 1.5629560586199827, 1.5536189829756744, 1.550888689145709, 1.5449470109981942, 1.5427414640134536, 1.5301536661341215, 1.5283940457241498, 1.5196441930069613, 1.5203789396066172, 1.5117691665615165, 1.510454159533505, 1.5025058051258682, 1.4969409059192735, 1.4902004661243338, 1.4877795388588695, 1.4819767493571476, 1.4761279459621965, 1.47296383896895, 1.4685310615524323, 1.46114164792212, 1.4543629032828123, 1.4518303447606922, 1.4499923410439648]\n"
     ]
    }
   ],
   "source": [
    "for dataframe in dataframes:\n",
    "    text = generate_corpus(dataframe)\n",
    "    artist = dataframe.iloc[0]['artist'].lower().replace(' ', '_')\n",
    "    chars, char_indices, sentences, next_chars = prep_text_for_training(text, maxlen=60, step=3)\n",
    "    x, y = vectorize_text(chars, char_indices, sentences, next_chars)\n",
    "    \n",
    "    model = get_model()\n",
    "#     os.mkdir(artist)\n",
    "    trained_model, history = train_model(model, x, y, path=os.path.join(artist, 'model.h5'))\n",
    "    print(f'{artist} training loss: {history.history[\"loss\"]}')\n",
    "    \n",
    "    with open(os.path.join(artist, 'chars.pkl'), 'wb') as f:\n",
    "        pickle.dump(chars, f)\n",
    "    with open(os.path.join(artist, 'char_indices.pkl'), 'wb') as f:\n",
    "        pickle.dump(char_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated_text = text[start_index: start_index + maxlen]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "for temperature in [0.7]:\n",
    "    print('------ temperature:', temperature)\n",
    "    sys.stdout.write(generated_text)\n",
    "\n",
    "    for i in range(400):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/char_indices.pkl', 'rb') as f:\n",
    "    char_indices = pickle.load(f)\n",
    "with open('models/chars.pkl', 'rb') as f:\n",
    "    chars = pickle.load(f)\n",
    "model = keras.models.load_model('models/rap.h5')\n",
    "\n",
    "maxlen = 60\n",
    "seed_text = 'somebody once told me the world is gonna roll me i aint the sharpest tool in the shed'\n",
    "generated_text = seed_text.lower()\n",
    "seed_text = ''.join(filter(lambda char: char if char in set(chars) else '', seed_text))\n",
    "seed_text = seed_text[-maxlen:]\n",
    "for i in range(400):\n",
    "    sampled = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(seed_text):\n",
    "        sampled[0, t, char_indices[char]] = 1\n",
    "\n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = sample(preds, temperature=0.75)\n",
    "    next_char = chars[next_index]\n",
    "\n",
    "    seed_text += next_char\n",
    "    seed_text = seed_text[1:]\n",
    "\n",
    "    generated_text += next_char"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
